
'matches': [
{

            {'id': 'md_5',
              'metadata': {'category': 'obsidian_note',
                           'filename': 'North Star This isn’t magic — it’s '
                                       'probabilities, guardrails, and logs',
                           'text': '**North Star:** This isn’t magic — it’s '
                                   'probabilities, guardrails, and logs. Tags '
                                   'like `insight-only`, `real-talk`, and '
                                   '`safe-educational` don’t change the model, '
                                   'they tilt the context so responses pull '
                                   'from different slices of training. The '
                                   'system will always be bounded, monitored, '
                                   'and probability-driven. The value is not '
                                   'in “hacking” it, but in integrating it: '
                                   'use it to accelerate coding, research, '
                                   'writing, and analysis. Trust it for speed '
                                   'and patterns, not for ultimate truth. Ride '
                                   'it like a co-pilot with limited vision: '
                                   'fast, helpful, not omniscient. The '
                                   'mind-fuck is real, but the power is in '
                                   'knowing when to lean on it, when to step '
                                   'back, and how to stay safe while doing '
                                   'both.'
                },
              'score': 0.23341848,
              'values': []
            },
            {'id': 'md_4',
              'metadata': {'category': 'obsidian_note',
                           'filename': '02_transformer_mechanics_EN',
                           'text': '# Transformer mechanics — shapes, passes, '
                                   'and the 30‑D intuition\n'
                                   '\n'
                                   '## The forward pass at a glance\n'
                                   '**Encoder layer** (repeated):\n'
                                   '1) **Self‑attention** over the input '
                                   'tokens (mix information across the '
                                   'sequence).  \n'
                                   '2) **Feed‑forward network (FFN)** per '
                                   'token (two linear layers with a '
                                   'nonlinearity).  \n'
                                   'Each sublayer is wrapped with **residual '
                                   'connections** and **layer norm**.\n'
                                   '\n'
                                   '**Decoder layer** (repeated):\n'
                                   '- **Masked self‑attention** (can’t peek at '
                                   'future tokens).  \n'
                                   '- **Cross‑attention** over the encoder '
                                   'outputs.  \n'
                                   '- **FFN**, plus residuals + layer norm.\n'
                                   '\n'
                                   '> Many modern LLMs (GPT‑style) use only '
                                   'the **decoder** stack; BERT‑style models '
                                   'use only the **encoder** stack.\n'
                                   '\n'
                                   '## Shapes (keep it concrete)\n'
                                   'Assume batch size **B**, sequence length '
                                   '**T**, model width **D** (say **D=30** as '
                                   'a mental image):\n'
                                   '- Input embeddings: **(B, T, D)**  \n'
                                   '- Projections to **Q, K, V**: three weight '
                                   'matrices of shapes **(D, dₖ)**, **(D, '
                                   'dₖ)**, **(D, dᵥ)** (often **dₖ = dᵥ = D / '
                                   'heads**)  \n'
                                   '- Attention scores: **(B, heads, T, T)** '
                                   'from `Q @ Kᵀ / √dₖ`  \n'
                                   '- Attention weights: **softmax(scores)**, '
                                   'same shape  \n'
                                   '- Context: `weights @ V` → **(B, heads, T, '
                                   'dᵥ)** → concat heads → **(B, T, D)** → FFN '
                                   '→ next layer\n'
                                   '\n'
                                   '## Your 30‑D dot‑product picture\n'
                                   'For one head and one batch element: each '
                                   '**qᵢ ∈ R³⁰** takes a **dot product** with '
                                   '**every** **kⱼ ∈ R³⁰** for j=1..T → a '
                                   'length‑T score vector. **Softmax** turns '
                                   'that into “who matters how much,” and we '
                                   'mix **V** with those weights.\n'
                                   '\n'
                                   '> Frameworks implement this as **matrix '
                                   'multiplies** (BLAS/cuBLAS), not Python '
                                   'loops—hence the speed on GPU/TPU.\n'
                                   '\n'
                                   '## Complexity and long context\n'
                                   'Self‑attention compares **all pairs** → '
                                   '**O(T²)** memory/compute. Long contexts '
                                   'get expensive, so you’ll see optimizations '
                                   'like **FlashAttention**, **sparse/linear '
                                   'attention**, and **KV caching** for faster '
                                   'generation.\n'
                                   '\n'
                                   '## Training signals\n'
                                   '- **Tokenization**: break text into '
                                   'subword IDs.  \n'
                                   '- Objectives: **next‑token prediction** '
                                   '(decoder‑only), **masked‑token '
                                   'prediction** (encoder‑only), etc.  \n'
                                   '- **Loss**: cross‑entropy over the '
                                   'vocabulary at each position.  \n'
                                   '- **Optimizer**: Adam/AdamW with '
                                   '**learning‑rate warmup** then decay.  \n'
                                   '- **Regularization**: dropout, label '
                                   'smoothing; often weight decay and gradient '
                                   'clipping.\n'
                },
              'score': 0.168251991,
              'values': []
            }
        ],
 'namespace': '',
 'usage': {'read_units': 1
        }
    }
